{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Foundations of Distributed Deep Learning using Azure Machine Learning"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In recent years, we have witnessed a significant surge in the size and complexity of Large Language Models (LLMs) and their associated technologies. These advancements have revolutionized various fields, from natural language processing to artificial intelligence, enabling machines to understand and generate human-like text with remarkable accuracy. However, this progress comes with its own set of challenges. One of the most pressing issues is the immense computational power required to train these models. It has become evident that a single GPU, regardless of its capabilities, is insufficient to train these models within a reasonable timeframe that aligns with market demands.\n",
        "\n",
        "To address this challenge, organizations are increasingly adopting the use of multiple GPUs in a single training run, a technique known as Distributed Deep Learning. This approach leverages the combined power of several GPUs to accelerate the training process, making it feasible to handle the vast amounts of data and complex computations involved in training LLMs. By distributing the workload across multiple GPUs, organizations can achieve faster training times and improve their time-to-market, which is crucial in the competitive landscape of AI development.\n",
        "\n",
        "In this lab, we will walk through the concepts that allow a Deep Learning (DL) framework to utilize multiple GPUs during a single training run. We will begin with simple jobs to introduce the fundamental concepts of distributed computing and data-parallel training. These initial exercises will help you understand how tasks are divided and managed across multiple GPUs, ensuring efficient utilization of resources.  We will then see how everything fits together for a training run of the MNIST dataset and model.  Finally, we will use the Pytorch Lightning library to see what real-world distributed training code looks like.\n",
        "\n",
        "It is important to note that this lab will focus on the mechanics of using multiple GPUs for distributed training rather than delving into specific deep learning techniques. The goal is to equip you with the knowledge and skills needed to set up and execute distributed training runs, laying a solid foundation for further exploration and application of advanced deep learning models.\n",
        "\n",
        "## Prerequisties\n",
        "In order to run this lab you will need:\n",
        "\n",
        "* An Azure Machine Learning (AML) workspace \n",
        "* The Azure Commnad-Line Interface (CLI), available for download [here](https://learn.microsoft.com/en-us/cli/azure/)\n",
        "* A basic understanding of Python and [PyTorch](https://pytorch.org/).  (Note: You do not need PyTorch installed on your local machine as all execution will take place on AML machines.)\n",
        "\n",
        "## Tools Used\n",
        "* Azure Machine Learning jobs and [serverless compute](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-use-serverless-compute?view=azureml-api-2&tabs=python)\n",
        "* PyTorch along with the [PyTorch Lightning](https://lightning.ai/docs/pytorch/stable/starter/introduction.html) library\n",
        "* The [Azure CLI's AML support](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-model?view=azureml-api-2&tabs=python)\n",
        "\n",
        "We will do most of our work from a Linux or WSL command-line shell so go ahead and open one now.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup \n",
        "\n",
        "In order to setup for this lab you will need to:\n",
        "\n",
        "1.  Open a command-line shell\n",
        "2.  Install the Azure CLI (see above)\n",
        "3.  Modify the following values and copy them into the shell.  These will allow the AML CLI to run the jobs in your workspace.\n",
        "4.  Make sure that the shell is open in the top level directory of this lab's repository \n",
        "\n",
        "The values in step #3 above can be found on the top-right corer, next to your profile initials."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "export SUBSCRIPTION_ID=\"<YOUR SUBSCRIPTION ID\"\n",
        "export RESOURCE_GROUP=\"<RESOURCE GROUP IN WHICH YOUR WORKSPACE IS LOCATED\"\n",
        "export WORKSPACE_NAME=\"<AML WORKSPACE NAME\"\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 1 - How to Use Multiple Compute Units is a Single Job\n",
        "\n",
        "In this section we will get accquainted with the environment variables that AML assigns to different compute units in a single job.  The purpose of this assignment is so that different compute units (CPUs, GPUs, etc.) know where and how to talk to other units.  By making sure that each unit is uniquely identifiable, the DL framework can then assign different parts of the training to it.  We will begin by using CPU-based machines as they are cheap and plentiful and can easily explain the required points.\n",
        "\n",
        "In order to run an AML job from the command line, we need a _job description_ file with a `.yaml` extension.  Open the `jobs/simple_environment.yaml` file from the lab's repository in a text editor of your choice.  Since this is a fairly simple job that only uses OS commands and no Python code, we are able to provide the commands that AML will run directly in the job file.\n",
        "\n",
        "There are a few interesting point to note about this file:\n",
        "\n",
        "#### Job Command\n",
        "\n",
        "In line 2, prefixed by the `command:` key, we have the actual command to be run on AML's compute resources.  In this job we ask for environment variables of the running process, sort them and then filter them out to only show the ones we're interested in.  \n",
        "\n",
        "#### Environment Image\n",
        "\n",
        "When AML runs jobs, it does so in the context of _environments_, which are Docker containers.  By encapsulating the specific libraries and tools required for each job, AML can re-use the same compute resources across multiple jobs.  You can learn more about AML environments [here](https://learn.microsoft.com/en-us/azure/machine-learning/concept-environments?view=azureml-api-2).  For this lab we will be using pre-prepared environments.  \n",
        "\n",
        "In line 4, we specify our environment as the latest Python Docker container without any modifications.  AML will run this container and run our command (line 2) inside it.  \n",
        "\n",
        "#### Distribution \n",
        "\n",
        "In line 5 we have the _distribution_ section, which instructs AML on how to run the distributed training job.  Here we see that we ask for a `pytorch` distribution type and 3 processes on each node.  The distribution type determines the type of environment variables that AML makes available to the processes makin up the training job (see below).  Current possible values are `pytorch' and 'mpi' and they differ in the number and names of these environment variables.  For this job, we instruct AML to use 3 training processes on each node, for a total of 3 x (number of nodes) processes.\n",
        "\n",
        "#### Resources\n",
        "\n",
        "Immediately below the distribution, on line 8, we have our resource declaration.  We ask for 2 nodes of type `DS3_V2`, which is a VM with 4 CPU cores.  Having asked for 3 processes per node in the distribution section above, it follows that we will have a total of 2 x 3, or 6, processes across all nodes, each running the command.  The use of the resources section also implies that we using _serverless_ compute - that is, the resources are allocated _on-demand_ and are shut down after the job completes.  \n",
        "\n",
        "\n",
        "### Running the Code\n",
        "From your shell, run the following command:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az ml job create --file jobs/simple_environment.yaml --web \\\n",
        "    --resource-group ${RESOURCE_GROUP} --workspace-name ${WORKSPACE_NAME}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "AML will now start running the job and open a web browser to a status screen reporting the status of the job.  In the background the new VMs are being allocated and the job run.  In a few minutes your job should report its status as `Running` and then `Completed`.\n",
        "\n",
        "Once the job has completed, click the `Outputs and logs` below the job name and open the `user_logs` folder.  Your screen should resemble the following:\n",
        "\n",
        "\n",
        "\n",
        "![](./pics/outputs_and_logs.png)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see, we have 6 files, labeled `std_log_process_0.txt` through `std_log_process_5.txt`, corresponding to the 6 processes we were expecting.  Each such file is the standard output of the relevant process and AML has collected it from all of the nodes in the job automatically.  Looking inside these files, we can find some interesting environment variables:\n",
        "\n",
        "* RANK:  This is the index of the current process across *all* processes, regardless on what node they run - in other words, a *global* rank\n",
        "* LOCAL_RANK:  This is the index of the current process across all of the processes on its *current* node\n",
        "* NODE_RANK:  Similar to process ranks, each node has an index across all nodes of the job\n",
        "* MASTER_ADDR and MASTER_PORT  - The IP address and port to use for communication (see below)\n",
        "* WORLD_SIZE  - The total number of processes in the job\n",
        "\n",
        "So for our 2-node, 3-process-per-node job we have the following process layout:\n",
        "\n",
        "![](./pics/ranks.png)\n",
        "\n",
        "**Exercise**:  Go through some of the log files and make sure you understand the global and local ranks and on what nodes they ran.\n",
        "\n",
        "AML further provides the AZUREML_NODE_COUNT environment variable so that we can easily compute the global and local rank for each process in the job.  By assigning each process a unique RANK it is now possible to run the *same* script multiple times across multiple compute units and nodes and have the processes work together to complete a task in parallel.  We will see an example in the next section.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 2 - Communicating Between Ranks of a Single Job\n",
        "\n",
        "In this section we will see how to communicate between two ranks of a single job.  We will run two nodes with a single rank each and pass a message from rank 0 to rank 1 using PyTorch's `distributed` package.  Once more, we will use cheap CPU machines to perform this task and move to GPUs in the next section.\n",
        "\n",
        "We will now be using both a `.yaml` file and a python script.  Open `jobs/simple_node_communication.yaml` and `src/simple_node_communication.py` from the lab's repository in a text editor of your choice.  \n",
        "\n",
        "The .yaml file should look familiar with a few changes:\n",
        "\n",
        "*  We are running 4 processes in the job, with 4 nodes and a single process on each.  Again, we use serverless compute.\n",
        "*  We change the environment to use a container that has been prepared by Microsoft with CUDA and PyTorch.  \n",
        "\n",
        "The python file is much more interesting.  Let's go over the sections denoted by each numbered comment:\n",
        "\n",
        "#### Comment 1\n",
        "As we've seen before, AML will give each instance of a running command or script a set of environment variables.  In this section we extract the (global) rank as well as the world size, convert them to int and print them to the standard output, where we can then look at them in the AML job's status window.\n",
        "\n",
        "#### Comment 2\n",
        "In this section we initialize PyTorch's a distributed process group.  This construct, from PyTorch's `distributed` package, sets up the necessary scaffolding for individual ranks to communicate.  As we are using CPUs for our compute units we choose the `Gloo` (see [here](https://github.com/facebookincubator/gloo)) backend for communication.  Gloo is a _collective communications library_, which is a set of routines for compute units to efficiently communicate between each other.  For example, it implements algorithms for one compute unit to broadcast to all others or for synchronizing the contents of all compute units using a specific mathematical operation - such as when averaging out the different gradients stored all compute units together.\n",
        "\n",
        "\n",
        "#### Comment 3\n",
        "In this section we prepare the object we want to send between ranks.  Specifically, we _broadcast_ a string - in other words, a list of characters - from rank 0 to all other ranks.  We first initialize a list with a None value that will be the initial value held by each rank.  We will want this value to get _overriden_ in ranks other than 0.  Next, we replace the None with a string value only in rank 0.\n",
        "\n",
        "\n",
        "#### Comment 4\n",
        "In this section we perform the actual call to the communications routine.  We pass the object we wish to send (a list, in this case, hence the _object_list_ suffix on the function name) and the source rank.  As each rank executes the same code, this function has the effect of _sending_ on rank 0 and _receiving_ on all other ranks, performing a _one-to-many send_ or _broadcast_.  We then print the first element of the list on each rank and verify that all ranks have in fact received the message.\n",
        "\n",
        "#### Comment 5 \n",
        "In this section we shut down the distributed communications group and clean up our resources.\n",
        "\n",
        "\n",
        "\n",
        "### Running the Code\n",
        "From your shell, run the following command:\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az ml job create --file jobs/simple_node_communication.yaml --web \\\n",
        "    --resource-group ${RESOURCE_GROUP} --workspace-name ${WORKSPACE_NAME}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:\n",
        "\n",
        "1.  Change the number of nodes and processes per node and re-run the job.  Verify that the various rank layouts can receive broadcasts.\n",
        "\n",
        "2.  Change the code to send the message from a different rank\n",
        "\n",
        "By using broadcast and other collective communications primitives we are able to have individual ranks communicate as part of a single job.  So far, we have used the Gloo backend and CPUs to do this communication.  In the next section we will switch to GPUs."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 3 - Communicating Between Ranks of a Single Job on GPUs\n",
        "\n",
        "In this section we will once more run a job where two ranks communicate.  This time, though, we will use GPUs as our compute units and [NCCL](https://developer.nvidia.com/nccl), the Nvidia Collective Communications Library as the backend.  We will use _low priority_ (or _spot_) machines for our cluster in order to save on costs.\n",
        "\n",
        "We will once more be using both a `.yaml` file and a python script.  Open `jobs/simple_gpu_communication.yaml` and `src/simple_gpu_communication.py` from the lab's repository in a text editor of your choice.  \n",
        "\n",
        "The .yaml file now contains a new key:  `queue_settings` with a sub-key of `job_tier` and a value of `spot`.  AML will now try to allocate the GPU machines from the excess capacity available in the data center, thus saving on costs.  Note that this script may fail to allocate the nodes on the first try and/or may require multiple execution until these machines are available.\n",
        "\n",
        "Let's look at the python file.  It should seem very similar to the last, CPU-based version with a few small changes.\n",
        "\n",
        "#### Comment 1\n",
        "We once more extract the rank and world size that is provided by AML.  This time we also ask for the _local rank_, as we'll be needing it for choosing a GPU.\n",
        "\n",
        "#### Comment 2\n",
        "We initialize the distributed process group again, this time specifying `NCCL` as the backend.  This implies that we will be running the communications code on the GPU and therefore each process should be running on a single GPU.  \n",
        "\n",
        "#### Comment 3\n",
        "We now prepare the message - a torch Tensor object - to send to the other ranks.  Since both the message and the communications code needs to run on the GPU we first set the current CUDA device - i.e., the GPU that will be used unless explicitly specified otherwise.  On multi-GPU machines this means that different ranks will need a different notion of what 'current' means.  So we need to use the **local** rank (i.e., the index of the rank on the current node) to decide what GPU will be the current one for the rank.  We then upload the data to this current GPU.\n",
        "\n",
        "### Running the Code\n",
        "From your shell, run the following command:\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az ml job create --file jobs/simple_gpu_communication.yaml --web \\\n",
        "    --resource-group ${RESOURCE_GROUP} --workspace-name ${WORKSPACE_NAME}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 4 - Running a Distributed Neural Network on GPUs\n",
        "\n",
        "In this section we run both the forward and backward pass of a simple neural network.  We use data from the [MNIST](https://en.wikipedia.org/wiki/MNIST_database) dataset, run them forwards through a PyTorch model, compute the loss and update the model's parameters.  When training in a _distributed_ manner we add an additional step:  we calculate the _average_ of all parameters across _all GPUs_.   We now have identical parameter values in each GPUs and can continue with the next batch of data.\n",
        "\n",
        "This mode of distributed training is known as _Data Parallel_ (DP) and it entails the following:\n",
        "\n",
        "1.  Each GPU has a copy of the model and the challenge is to make sure all these copies have identical values after each batch of data is processed.  \n",
        "\n",
        "2.  Each GPU processes _different_ batches of data.  \n",
        "\n",
        "In this way n GPUs can process $n$ batches of data in parallel and training time is $1/n$ (in theory - in practice not everything can be parallelized).  Note that processing $n$ batches at the same time means that the _effective batch size_ is actually $N \\cdot batch size$ and adjustments in learning rate may be needed.\n",
        "\n",
        "We will once more be using both a `.yaml` file and a python script.  Open `jobs/mnist_all_reduce.yaml` and `src/mnist_all_reduce.py` from the lab's repository in a text editor of your choice. \n",
        "\n",
        "The .yaml file should look familiar.  Make sure you understand what each line does.\n",
        "\n",
        "Let's now look at the python file.  We use `torchvision`, which is a PyTorch-based library for computer vision-related tasks.  In particular, we use the `MNIST` class which enhances PyTorch's `Dataset` class by giving us an easy way to download the data.  We will also be be using some new distributed operations.\n",
        "\n",
        "#### Comment 1\n",
        "\n",
        "Since we may be running the job on multiple nodes (see the `instance_count` key in the .yaml file) we need to make sure that each node has a copy of the MNIST dataset.  Note that in large clusters, typically a _parallel file system_ such as [Azure Managed Lustre](https://learn.microsoft.com/en-us/azure/azure-managed-lustre/amlfs-overview) is used in order to avoid multiple such downloads.  In our case, though, we need to download the data *just once* on each node.  We therefore condition the download to run only on processes that have _local rank_ of 0.  However, the question arises - how can we make all the other ranks wait until this download is complete?  For that we use the _barrier_ operation from `torch.distributed`.  A barrier acts as a _synchronization point_:  it waits until _all_ ranks arrive at this code before continuing.  In this way we can make sure that the download is complete before any additional processing is done.\n",
        "\n",
        "\n",
        "#### Comment 2\n",
        "\n",
        "Now that the data has been downloaded we can use the `MNIST` class once more to load the data as a PyTorch dataset.  The next step is to pass this dataset to an instance of the `DataLoader` from which we can get individual data batches.  Once more, we need to take into account the fact that we're running in a data-parallel fashion:  each one of the $n$ ranks needs to have its own data.  For that we use PyTorch's  `DistributedSampler` class that can access the current process's rank as well as the overall world size and ensure unique data batches for each rank.  We pass this sampler as an additional parameter to the data loader.  Also note that the sampler requires the `shuffle` parameter to be False - it takes care of this functionality on its own.  For demonstration purposes we will run a single batch with a batch size of 1 - so only a single data point.\n",
        "\n",
        "\n",
        "#### Comment 3\n",
        "\n",
        "We now print the rank and the label of the data item.  Since each rank has its own data, you should see different labels, although you may get data items with the same label by chance.  If so, just keep reading - we'll see the actual data in a bit.  \n",
        "\n",
        "\n",
        "#### Comment 4\n",
        "\n",
        "By this point we've already created a small feed-forward network and executed a standard PyTorch training loop that includes a forward and backward pass.  We now print the first 10 weights in the network along with the rank.  As each rank trained the same model (i.e., identical weights) on different data, these weights should be different for each rank. In other words - we have $n$ different models for $n$ different ranks and we now need to _average_ them across all of the ranks.  \n",
        "\n",
        "\n",
        "#### Comment 5\n",
        "\n",
        "We now use the _all-reduce_ operation from `torch.distributed` to collect the values for each parameter in the model, average (OpReduce.AVG) them and send them back to each GPU.  At the end of this operation each GPU will have byte-identical values of the parameters.  As you can imagine, for large models this is a costly and time-consuming operation that is _communications-bound_ rather than _compute-bound_, thereby making distributed training a High-Performance Computing (HPC)-type problem.  Collective communications libraries such an NCCL (or [MSCCL](https://arxiv.org/abs/2201.11840v1), Microsoft's Azure-optimized version of NCCL) will look for the fastest path available between GPUs, such as [NVLink and NVSwitch](https://www.nvidia.com/en-us/data-center/nvlink/) for intra-node communications and [Infiniband](https://network.nvidia.com/pdf/whitepapers/IB_Intro_WP_190.pdf) for inter-node communications.  These two features are the hallmark of Microsoft's Azure [ND-family](https://learn.microsoft.com/en-us/azure/virtual-machines/sizes/gpu-accelerated/nd-family) of GPU-accelerated virtual machines.  \n",
        "\n",
        "\n",
        "#### Comment 6\n",
        "\n",
        "Similar to comment 4 above, we now print the first 10 weights in the model again.  This time, they should be identical across all ranks and each weight is the average of all weights at the same location above.  Once more, we have $n$ identical copies of the model and can repeat the process with a new data batch.\n",
        "\n",
        "\n",
        "\n",
        "### Running the Code\n",
        "From your shell, run the following command:\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az ml job create --file jobs/mnist_all_reduce.yaml --web \\\n",
        "    --resource-group ${RESOURCE_GROUP} --workspace-name ${WORKSPACE_NAME}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part 5 - Running Real-World Distributed Training on GPUs\n",
        "\n",
        "In this section we will be running a fully distributed, multi-node, GPU training job with the MNIST dataset.  Distribution across multiple nodes will be handled by the open-source PyTorch Lightning library which is a useful wrapper on top of plain PyTorch, offering a number of benefits such as:\n",
        "\n",
        "*  A `Trainer` object that encapsulates the training and validation loops as well as number of required nodes and ranks on each node \n",
        "*  A `LightningModule` base class with well-defined, overridable methods for different parts of the training flow\n",
        "\n",
        "For actual, real-world scenarios a library such as Pytorch Lightning is an invaluable tool, allowing you to focus on your task rather than on infrastructure and low-level details. \n",
        "\n",
        "\n",
        "\n",
        "### Running the Code\n",
        "From your shell, run the following command:\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "az ml job create --file jobs/mnist_ddp.yaml --web \\\n",
        "    --resource-group ${RESOURCE_GROUP} --workspace-name ${WORKSPACE_NAME}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise**:  Try increasing the number of nodes and the processes per node and re-running the job"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Summary\n",
        "\n",
        "In this lab, we delved into the foundational concepts of distributed deep learning, a critical area for scaling machine learning models across multiple devices and nodes. We began by exploring how Azure Machine Learning (AML) assigns ranks to individual processes, which is essential for managing and coordinating tasks in a distributed environment. This ranking system ensures that each process knows its role and can effectively contribute to the overall computation.\n",
        "\n",
        "Next, we examined the use of collective communications libraries, which are pivotal for enabling efficient communication between processes. These libraries, when used in conjunction with `pytorch.distributed`, facilitate the exchange of information and synchronization among processes. This step is crucial for maintaining consistency and accuracy in the distributed training process.\n",
        "\n",
        "Finally, we integrated these concepts using PyTorch Lightning, a high-level framework that simplifies the implementation of complex training workflows. By wrapping our distributed training setup with PyTorch Lightning, we were able to streamline the process, making it more manageable and easier to execute a full training run. This approach not only enhances productivity but also ensures that best practices are followed, leading to more robust and scalable deep learning models.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {}
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}