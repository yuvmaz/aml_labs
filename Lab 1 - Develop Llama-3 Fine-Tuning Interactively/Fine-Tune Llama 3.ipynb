{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning Llama 3 Using Azure Machine Learning Notebooks and Hugging Face Tools"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this lab we will fine-tune a Llama 3 model using tools, models and datasets from Hugging Face on an Azure Machine Learning (AML) GPU instance.  In particular, we will perform Low-Rank Approximation (LoRA) training which can run on even a smaller GPU such as the T4 using the [PyTorch](https://pytorch.org/) deep-learning framework.\n",
        "\n",
        "We will fine-tune the model using a dataset consisting of medical questions and answers.  In essence, we will be creating a 'Llama 3 Doctor' model that has specific knowledge of the medical domain.  \n",
        "\n",
        "## Prerequisites\n",
        "To run this lab you need to have the following:\n",
        "* An AML GPU-based compute instance\n",
        "* A conda environment named `llama3_ft`, created using the enclosed `1_setup_conda.sh` file\n",
        "* The [Medical Dialogs](https://huggingface.co/datasets/ruslanmv/ai-medical-chatbot) dataset, loaded into AML's Data Repository, accessible through the Assets->Data link on the left-hand side of the screen, with the name `MedicalDialogs`.\n",
        "\n",
        "## Tools Used\n",
        "The Python tools used in this lab are the following open-source Hugging Face tools:\n",
        "\n",
        "* [Transformers](https://huggingface.co/docs/transformers/v4.17.0/en/index) - Implementation of a number of deep-learning models using the Transformer architecture\n",
        "* [PEFT](https://huggingface.co/docs/peft/index) - Implementation of Parameter-Efficient Fine-Tuning, which allows the fine-tuning of pretrained models using only a small subset of their parameters.  We will be using the Quantized LORA (QLORA) algorithm for fine-tuning a model that was quantized to use 4 bits for each weight instead of 16 bits. \n",
        "* [TRL](https://huggingface.co/docs/trl/index) - This library contains a number of algorithms that help train Transformer-based language models using Reinforcement Learning.  As our dataset contains medical questions and answers, we will be using the Supervised Fine-Tuning (SFT) algorithm.\n",
        "* [Accelerate](https://huggingface.co/docs/accelerate/index) - This library makes it easy to run multi-gpu training and is integrated into the other libraries we will use.\n",
        "* [BitsAndBytes](https://huggingface.co/docs/bitsandbytes/index) - This library provides tools for loading models in a quantized form for PyTorch. \n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Working directory on the AML compute instance\n",
        "!pwd "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/mnt/batch/tasks/shared/LS_root/mounts/clusters/t4-instance/code/Users/yuvalmazor/FT Llama 3\r\n"
        }
      ],
      "execution_count": 1,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736835958277
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports & Definitions\n",
        "In this section we will import the classes we need and setup some definitions to be used later on."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    HfArgumentParser,\n",
        "    TrainingArguments,\n",
        "    pipeline,\n",
        "    logging,\n",
        ")\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    PeftModel,\n",
        "    AutoPeftModelForCausalLM,\n",
        "    prepare_model_for_kbit_training,\n",
        "    get_peft_model,\n",
        ")\n",
        "\n",
        "import os\n",
        "import torch\n",
        "\n",
        "from datasets import load_dataset, Dataset, ReadInstruction\n",
        "from trl import SFTTrainer, setup_chat_format, SFTConfig\n"
      ],
      "outputs": [],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1736835982407
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Name of base model to use for fine-tuning, retrieved from HuggingFace\n",
        "base_model = \"NousResearch/Meta-Llama-3-8B\"\n",
        "\n",
        "# Name and version of dataset, to be retrieved from AML's Data Repository\n",
        "finetune_dataset = \"MedicalDialogs\"\n",
        "finetune_dataset_version = \"1\"\n",
        "\n",
        "# How many samples to use from the dataset\n",
        "finetune_dataset_samples = 1000\n",
        "\n",
        "# Size of test (evaluation) set, of the total number of samples\n",
        "test_size = 0.1\n",
        "\n",
        "# Directory name in which to save the LoRA adapters for the fine-tuned model\n",
        "finetuned_model = \"llama3-8b-chat-doctor\"\n",
        "\n",
        "# Directory name in which to save the configuration settings for the fine-tuned model\n",
        "finetuned_model_config = f\"{finetuned_model}_config\"\n",
        "\n",
        "# Directory name in which to save the full fine-tuned model\n",
        "finetuned_model_full = f\"{finetuned_model}_full\"\n",
        "\n",
        "# Directory in which to save model files downloaded from HuggingFace\n",
        "cache_dir = \"/mnt/tmp/hf_cache\"\n",
        "\n",
        "# Sample question to ask the final fine-tuned model\n",
        "messages = [\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"Hello doctor, I get red blotches on my skin whenever I'm next to a cat.  What can I do?\"\n",
        "    }\n",
        "]\n"
      ],
      "outputs": [],
      "execution_count": 3,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736835982583
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Model\n",
        "In this section we load the model and tokenizer, performing the 4-bit quantization and moving the model to the GPU.  In addition, we setup the model and tokenizer to automatically add the chat template to the data.  The _chat template_ is the addition of custom markup tokens so the model is able to distinguish between roles, content and other types of information."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Setup 4-bit quantization \n",
        "qlora_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "# Download the tokenizer from HuggingFace\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model)\n",
        "\n",
        "# Download the model from HuggingFace\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model,\n",
        "    # Apply quantization\n",
        "    quantization_config=qlora_config,\n",
        "    # Automatically load the model into the GPU\n",
        "    device_map=\"auto\",\n",
        "    attn_implementation=\"eager\",\n",
        "    cache_dir=cache_dir\n",
        ")\n",
        "\n",
        "# Ensure the model and tokenizer are setup to use the proper prefixes and suffixes required for \n",
        "# marking the role and content \n",
        "model, tokenizer = setup_chat_format(model, tokenizer)"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f47f01e9bfeb4224b54e0c530ea9f5ee"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\nThe new lm_head weights will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n"
        }
      ],
      "execution_count": 4,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736836009275
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup LoRA \n",
        "In this section we setup the LoRA configuration, and prepare the model for parameter-efficient fine-tuning."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "peft_config = LoraConfig(\n",
        "    r=4,\n",
        "    lora_alpha=16,\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\", \n",
        ")\n",
        "model = get_peft_model(model, peft_config)"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736836009409
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Retrieve and Prepare the Fine-Tuning Data\n",
        "In this section we:\n",
        "* Retrieve the medial dialogs dataset from AML's Data Repository\n",
        "* Sample from the dataset\n",
        "* Split the data into training and test (evaluation) sets\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from azure.ai.ml import MLClient\n",
        "from azure.identity import DefaultAzureCredential\n",
        "import pandas as pd\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "SUBSCRIPTION=\"eba489bb-ec02-466c-9806-a269d915d943\"\n",
        "RESOURCE_GROUP=\"yuvmaz-aml\"\n",
        "WS_NAME=\"yuvmaz-aml\"\n",
        "\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=SUBSCRIPTION,\n",
        "    resource_group_name=RESOURCE_GROUP,\n",
        "    workspace_name=WS_NAME\n",
        ")\n",
        "\n",
        "# Retrieve the dataset from AML where it is stored in Parquet format\n",
        "data_asset = ml_client.data.get(name=finetune_dataset, version=finetune_dataset_version)\n",
        "df = pd.read_parquet(data_asset.path)\n",
        "\n",
        "# Display a sample of the dataset \n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Resolving access token for scope \"https://storage.azure.com/.default\" using identity of type \"MANAGED\".\nGetting data access token with Assigned Identity (client_id=clientid) and endpoint type based on configuration\n"
        },
        {
          "output_type": "execute_result",
          "execution_count": 6,
          "data": {
            "text/plain": "                                         Description  \\\n0      Q. What does abutment of the nerve root mean?   \n1  Q. What should I do to reduce my weight gained...   \n2  Q. I have started to get lots of acne on my fa...   \n3  Q. Why do I have uncomfortable feeling between...   \n4  Q. My symptoms after intercourse threatns me e...   \n\n                                             Patient  \\\n0  Hi doctor,I am just wondering what is abutting...   \n1  Hi doctor, I am a 22-year-old female who was d...   \n2  Hi doctor! I used to have clear skin but since...   \n3  Hello doctor,I am having an uncomfortable feel...   \n4  Hello doctor,Before two years had sex with a c...   \n\n                                              Doctor  \n0  Hi. I have gone through your query with dilige...  \n1  Hi. You have really done well with the hypothy...  \n2  Hi there Acne has multifactorial etiology. Onl...  \n3  Hello. The popping and discomfort what you fel...  \n4  Hello. The HIV test uses a finger prick blood ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Description</th>\n      <th>Patient</th>\n      <th>Doctor</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Q. What does abutment of the nerve root mean?</td>\n      <td>Hi doctor,I am just wondering what is abutting...</td>\n      <td>Hi. I have gone through your query with dilige...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Q. What should I do to reduce my weight gained...</td>\n      <td>Hi doctor, I am a 22-year-old female who was d...</td>\n      <td>Hi. You have really done well with the hypothy...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Q. I have started to get lots of acne on my fa...</td>\n      <td>Hi doctor! I used to have clear skin but since...</td>\n      <td>Hi there Acne has multifactorial etiology. Onl...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Q. Why do I have uncomfortable feeling between...</td>\n      <td>Hello doctor,I am having an uncomfortable feel...</td>\n      <td>Hello. The popping and discomfort what you fel...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Q. My symptoms after intercourse threatns me e...</td>\n      <td>Hello doctor,Before two years had sex with a c...</td>\n      <td>Hello. The HIV test uses a finger prick blood ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 6,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736836017166
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sample the required number of data points - retrieve the 'Doctor' and 'Patient' fields from the dataset\n",
        "dataset = Dataset.from_pandas(df[['Patient', 'Doctor']].sample(finetune_dataset_samples))\n",
        "\n",
        "# Format the dataset - the Patient field is passed as the user query, while the Doctor field is the answer we expect\n",
        "# From the model\n",
        "def format_chat_template(row):\n",
        "    row_json = [{\"role\": \"user\", \"content\": row[\"Patient\"]},\n",
        "               {\"role\": \"assistant\", \"content\": row[\"Doctor\"]}]\n",
        "    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n",
        "    return row\n",
        "\n",
        "\n",
        "dataset = dataset.map(\n",
        "    format_chat_template,\n",
        "    num_proc=4,\n",
        ")\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "dataset = dataset.train_test_split(test_size=test_size)\n",
        "\n",
        "# Show the resulting dataset and split sizes\n",
        "dataset"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9cf0f7d30a324fde98fa587c9cf4094b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 7,
          "data": {
            "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['Patient', 'Doctor', '__index_level_0__', 'text'],\n        num_rows: 900\n    })\n    test: Dataset({\n        features: ['Patient', 'Doctor', '__index_level_0__', 'text'],\n        num_rows: 100\n    })\n})"
          },
          "metadata": {}
        }
      ],
      "execution_count": 7,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736836017911
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Show a sample data instance and the resulting markup to be fed to the model while fine-tuning\n",
        "from pprint import pprint\n",
        "pprint(dataset['train'][0])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "{'Doctor': 'has to be diagnosed first whether it is simple or cancerous by '\n           'biopsy or clinical examination,better show it to dermatologist and '\n           'start treatment,by the mean time can take tab-cetrizine 10 mg once '\n           'daily',\n 'Patient': \"my husband had a rash on his penis. it's red and itchy. and it's \"\n            'spread to his right upper eye lid and now a small spot on his '\n            'bicep. we thought it was a yeast infection but at this point we '\n            'are unsure since its spreading. he has used an anti fungal on his '\n            'penis 3-4 times in the last 2 days. What could this be?',\n '__index_level_0__': 31802,\n 'text': '<|im_start|>user\\n'\n         \"my husband had a rash on his penis. it's red and itchy. and it's \"\n         'spread to his right upper eye lid and now a small spot on his bicep. '\n         'we thought it was a yeast infection but at this point we are unsure '\n         'since its spreading. he has used an anti fungal on his penis 3-4 '\n         'times in the last 2 days. What could this be?<|im_end|>\\n'\n         '<|im_start|>assistant\\n'\n         'has to be diagnosed first whether it is simple or cancerous by '\n         'biopsy or clinical examination,better show it to dermatologist and '\n         'start treatment,by the mean time can take tab-cetrizine 10 mg once '\n         'daily<|im_end|>\\n'}\n"
        }
      ],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736836018043
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setup and Run the Training\n",
        "In this section we:\n",
        "\n",
        "* Perform the training, using HuggingFace TRL's Supervised Fine-Tuning (SFT) algorithm\n",
        "* Send a sample query to the fine-tuned model and observer the output\n",
        "* Save the LoRA adapter, the configuration and the full (merged) model"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training_arguments = TrainingArguments(\n",
        "training_arguments = SFTConfig(\n",
        "    output_dir=\"/tmp\",\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    gradient_accumulation_steps=2,\n",
        "    optim=\"paged_adamw_32bit\",\n",
        "    num_train_epochs=1,\n",
        "    eval_strategy=\"steps\",\n",
        "    eval_steps=0.2,\n",
        "    logging_steps=50,\n",
        "    warmup_steps=10,\n",
        "    logging_strategy=\"steps\",\n",
        "    learning_rate=2e-4,\n",
        "    fp16=True,\n",
        "    bf16=False,\n",
        "    group_by_length=True,\n",
        "    report_to='azure_ml',   \n",
        "    max_seq_length=512,\n",
        "    dataset_text_field=\"text\",\n",
        "    packing= False,\n",
        ")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736836018184
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    peft_config=peft_config,\n",
        "    tokenizer=tokenizer,\n",
        "    args=training_arguments,\n",
        ")\n",
        "\n",
        "stats = trainer.train()\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/tmp/ipykernel_22109/3334957991.py:1: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n  trainer = SFTTrainer(\n"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/900 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b19738bf8c13402fa65a0eedae20f900"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Map:   0%|          | 0/100 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "68bf784e15c94d3996f406da8cca1bbe"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [450/450 18:26, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>90</td>\n      <td>2.787300</td>\n      <td>2.646570</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.606200</td>\n      <td>2.625488</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>2.525400</td>\n      <td>2.608860</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>2.552800</td>\n      <td>2.605022</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>2.514500</td>\n      <td>2.600989</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Attempted to log scalar metric loss:\n2.7873\nAttempted to log scalar metric grad_norm:\n3.8621809482574463\nAttempted to log scalar metric learning_rate:\n0.00018318181818181817\nAttempted to log scalar metric epoch:\n0.1111111111111111\nAttempted to log scalar metric eval_loss:\n2.6465702056884766\nAttempted to log scalar metric eval_runtime:\n33.4522\nAttempted to log scalar metric eval_samples_per_second:\n2.989\nAttempted to log scalar metric eval_steps_per_second:\n2.989\nAttempted to log scalar metric epoch:\n0.2\nAttempted to log scalar metric loss:\n2.6119\nAttempted to log scalar metric grad_norm:\n2.6427183151245117\nAttempted to log scalar metric learning_rate:\n0.00016045454545454547\nAttempted to log scalar metric epoch:\n0.2222222222222222\nAttempted to log scalar metric loss:\n2.6062\nAttempted to log scalar metric grad_norm:\n5.086814880371094\nAttempted to log scalar metric learning_rate:\n0.00013772727272727274\nAttempted to log scalar metric epoch:\n0.3333333333333333\nAttempted to log scalar metric eval_loss:\n2.625488042831421\nAttempted to log scalar metric eval_runtime:\n33.6763\nAttempted to log scalar metric eval_samples_per_second:\n2.969\nAttempted to log scalar metric eval_steps_per_second:\n2.969\nAttempted to log scalar metric epoch:\n0.4\nAttempted to log scalar metric loss:\n2.5863\nAttempted to log scalar metric grad_norm:\n3.1706111431121826\nAttempted to log scalar metric learning_rate:\n0.00011499999999999999\nAttempted to log scalar metric epoch:\n0.4444444444444444\nAttempted to log scalar metric loss:\n2.5254\nAttempted to log scalar metric grad_norm:\n2.6565425395965576\nAttempted to log scalar metric learning_rate:\n9.227272727272727e-05\nAttempted to log scalar metric epoch:\n0.5555555555555556\nAttempted to log scalar metric eval_loss:\n2.6088602542877197\nAttempted to log scalar metric eval_runtime:\n33.668\nAttempted to log scalar metric eval_samples_per_second:\n2.97\nAttempted to log scalar metric eval_steps_per_second:\n2.97\nAttempted to log scalar metric epoch:\n0.6\nAttempted to log scalar metric loss:\n2.544\nAttempted to log scalar metric grad_norm:\n3.355464220046997\nAttempted to log scalar metric learning_rate:\n6.954545454545455e-05\nAttempted to log scalar metric epoch:\n0.6666666666666666\nAttempted to log scalar metric eval_loss:\n2.6050217151641846\nAttempted to log scalar metric eval_runtime:\n35.714\nAttempted to log scalar metric eval_samples_per_second:\n2.8\nAttempted to log scalar metric eval_steps_per_second:\n2.8\nAttempted to log scalar metric epoch:\n0.8\nAttempted to log scalar metric loss:\n2.5228\nAttempted to log scalar metric grad_norm:\n3.2871687412261963\nAttempted to log scalar metric learning_rate:\n2.4090909090909093e-05\nAttempted to log scalar metric epoch:\n0.8888888888888888\nAttempted to log scalar metric loss:\n2.5145\nAttempted to log scalar metric grad_norm:\n2.583381414413452\nAttempted to log scalar metric learning_rate:\n1.3636363636363636e-06\nAttempted to log scalar metric epoch:\n1.0\nAttempted to log scalar metric train_runtime:\n1109.7462\nAttempted to log scalar metric train_samples_per_second:\n0.811\nAttempted to log scalar metric train_steps_per_second:\n0.405\nAttempted to log scalar metric total_flos:\n9183754580582400.0\nAttempted to log scalar metric train_loss:\n2.5834727647569444\nAttempted to log scalar metric epoch:\n1.0\n"
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n/anaconda/envs/bert_ft/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n"
        }
      ],
      "execution_count": 10,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736837129847
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, max_length=512, truncation=True).to('cuda')\n",
        "\n",
        "outputs = model.generate(**inputs, max_length=150, num_return_sequences=1)\n",
        "text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "print(text.split(\"assistant\")[1])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "\nHi. For this, you need to take antihistamine tablet and apply a steroid cream on the blotches. I hope I have answered your query. Let me know if I can assist you further. Take care Regards, Dr. Pankaj M. Rathi, General & Family Physician, Internal Medicine Specialist, General Surgeon, General Physician, General Practitioner, Family Physician, General & Family Physician, General Medicine Specialist, Internal Medicine Specialist, General Surgeon, General Physician, General Practitioner, Family Physician, General & Family Physician, General Medicine Specialist, Internal\n"
        }
      ],
      "execution_count": 11,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736837137876
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.save_model(finetuned_model)\n",
        "peft_config.save_pretrained(finetuned_model_config)\n",
        "model.merge_and_unload().save_pretrained(finetuned_model_full)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/bert_ft/lib/python3.10/site-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n  warnings.warn(\n/anaconda/envs/bert_ft/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:355: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n  warnings.warn(\n"
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736837241457
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "llama3_ft",
      "language": "python",
      "display_name": "Python (llama3_ft)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "llama3_ft"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}