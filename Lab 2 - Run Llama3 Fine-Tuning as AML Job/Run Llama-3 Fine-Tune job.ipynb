{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning Llama 3 Using Azure Machine Learning Jobs and Hugging Face Tools "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this lab we will build upon the knowledge gained in the previous lab, where you fine-tuned Llama 3 inside a notebook.  For that lab, we used an interactive Jupyter notebook with one (possibly small) GPU to perform development.  \n",
        "\n",
        "In this lab we'll run the same code using an Azure ML _job_, which can be run asynchronously as well as scheduled to run at a specific time.  We will also use an AML _cluster_ which can contain multiple machines with multiple GPUs each.  This is an ideal approach for fine-tuning large LLMs and/or training for many epochs.\n",
        "\n",
        "## Prerequisites\n",
        "To run this lab you need to have the following:\n",
        "\n",
        "* An AML single-machine compute cluster named `mycluster`\n",
        "* An AML `environment` named `finetune_llama3@latest` and containing the same libraries as the ones in the `requirements.txt` file\n",
        "* The Medical Dialogs dataset, loaded into AML's Data Repository, accessible through the Assets->Data link on the left-hand side of the screen, with the name MedicalDialogs.\n",
        "* The code fine named `fine_tune_llama3_doctor.py` which is a slightly modified, single-file version of the code in the previous lab\n",
        "\n",
        "## Tools Used\n",
        "The Python tools used in this lab are the following open-source Hugging Face tools:\n",
        "\n",
        "* [Transformers](https://huggingface.co/docs/transformers/v4.17.0/en/index) - Implementation of a number of deep-learning models using the Transformer architecture\n",
        "* [PEFT](https://huggingface.co/docs/peft/index) - Implementation of Parameter-Efficient Fine-Tuning, which allows the fine-tuning of pretrained models using only a small subset of their parameters.  We will be using the Quantized LORA (QLORA) algorithm for fine-tuning a model that was quantized to use 4 bits for each weight instead of 16 bits. \n",
        "* [TRL](https://huggingface.co/docs/trl/index) - This library contains a number of algorithms that help train Transformer-based language models using Reinforcement Learning.  As our dataset contains medical questions and answers, we will be using the Supervised Fine-Tuning (SFT) algorithm.\n",
        "* [Accelerate](https://huggingface.co/docs/accelerate/index) - This library makes it easy to run multi-gpu training and is integrated into the other libraries we will use.\n",
        "* [BitsAndBytes](https://huggingface.co/docs/bitsandbytes/index) - This library provides tools for loading models in a quantized form for PyTorch. \n",
        "\n",
        "We will also use the built-in [MLFlow](https://mlflow.org/docs/latest/tracking.html) capabilites in AML to track metrics and outputs from our job.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports & Definitions\n",
        "\n",
        "In this section we import the required libraries for running the job.  Note that for this notebook we only use the AML SDK and a pre-configured Jupyter kernel that comes with AML.  The actual training code has been moved into `fine_tune_llama_3_doctor.py` with the addition of a `main` function and the ability to accept training parameters."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# AML SDK imports\n",
        "\n",
        "from azure.ai.ml import MLClient, Input, Output, command\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from azure.ai.ml.entities import JobResourceConfiguration\n",
        "\n",
        "\n",
        "# Authentication\n",
        "credential = DefaultAzureCredential()\n",
        "\n",
        "# AML Details - These are available by clicking the down-arrow on the top right-hand sign of the screen, next to \n",
        "# your login initials\n",
        "\n",
        "SUBSCRIPTION=\"<Your Subscription ID>\"\n",
        "RESOURCE_GROUP=\"<AML Studio Resource Group Name>\"\n",
        "WS_NAME=\"<Workspace Name>\"\n",
        "\n",
        "# Get a handle to the workspace - this handle allows you to send commands to the workspace via code\n",
        "ml_client = MLClient(\n",
        "    credential=credential,\n",
        "    subscription_id=SUBSCRIPTION,\n",
        "    resource_group_name=RESOURCE_GROUP,\n",
        "    workspace_name=WS_NAME\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "gather": {
          "logged": 1734933274163
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Submitting the Job \n",
        "\n",
        "In this section we submit the job for processing using the AML SDK.\n",
        "\n",
        "Some interesting points in this `command` object:\n",
        "*  We submit the entire directory in the AML notebook to the job\n",
        "*  We use Hugging Face's `accelerate` tool to automatically use all of the GPU's available on the compute cluster\n",
        "*  We define and submit a `batch_size` parameter, that will be handled in the `fine_tune_llama_3_doctor.py` file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Declare a command object with the details of the job\n",
        "job = command(\n",
        "\n",
        "    # local path where the code is stored - in this case, we want the entire directory in the AML notebook \n",
        "    code=\".\",  \n",
        "\n",
        "    # Using HuggingFace tools to request ALL GPUs on the machine\n",
        "    command=\"python download_files.py && accelerate launch fine_tune_llama_3_doctor.py \" + \\\n",
        "        \"--num_epochs=${{inputs.num_epochs}} --batch_size=${{inputs.batch_size}} --num_data_rows=${{inputs.num_data_rows}}\",\n",
        "\n",
        "    # Pass a custom parameter to the training script\n",
        "    inputs={\n",
        "        \"batch_size\": 8,\n",
        "        \"num_epochs\": 2,\n",
        "        \"num_data_rows\": 10000,\n",
        "    },\n",
        "\n",
        "    # Name of environment to use\n",
        "    environment=\"finetune_llama3@latest\",\n",
        "\n",
        "    # Display name of this experiment in the Jobs display (MLFlow)\n",
        "    display_name=\"Fine-Tune Llama 3 \",\n",
        "\n",
        "    # Name of compute cluster\n",
        "    compute=\"mycluster\",\n",
        "\n",
        "    # Directory (on host) where to save HF models\n",
        "    resources=JobResourceConfiguration(docker_args=\"-v=/mnt:/mounts\"),\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1734943400247
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run and return status for the command object\n",
        "returned_job = ml_client.create_or_update(job)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1734943424039
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Explanation of the Above Code\n",
        "\n",
        "In order to run an AML job, we upload a set of files which contain the code we developed in the previous lab, but this time as a standalone file.  We've also added:\n",
        "* A sepearate Python file for downloading the model and tokenizer files to a fixed location on the GPU machine.  This is to ensure that we only have to download these large files ones.\n",
        "* A `main` function to the previous code that can optionally receive parameters such as batch size, learning_rate, etc.   \n",
        "\n",
        "We also require an `environment`, which is just a Docker container with the proper libraries to run the standalone Python file.  This environment can be created in the *Assets -> Environments* tab on the left-hand side of the screen.\n",
        "\n",
        "When a job is submitted to AML using its SDK AML performs the following steps:\n",
        "1.  Downloads system containers and the environment container to the cluster machine.  This may take a few minutes when done for the first time on new compute cluster machines.\n",
        "\n",
        "2.  AML will run the standalone Python file containing the experiment code inside the environment's Docker container, while making sure it has access to an MLFLow server for tracking and all of the files in the directory specified in the command.  \n",
        "\n",
        "3.  AML will write the standard output and error streams of the run to `std_log.txt` in the working directory which can be viewed in AML under the `Jobs` tabs and clicking on the experiment name.\n",
        "\n",
        "4.  Anything written to the `output` directory in the working directory (such as the model file or any other additional files) can be retrieved from the same place.\n",
        "\n",
        "5.  Finally, all MLFlow metrics can be reviewed in the same place as well.\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}