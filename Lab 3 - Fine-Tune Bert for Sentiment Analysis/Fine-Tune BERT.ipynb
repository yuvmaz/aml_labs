{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-Tuning BERT Using Azure Machine Learning Notebooks and Hugging Face Tools"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction\n",
        "\n",
        "In this lab we will fine-tune a BERT model using tools, models and datasets from Hugging Face on an Azure Machine Learning (AML) GPU instance.  In particular, we will add a _classification head_ on top of a pre-trained instance of BERT and fine-tune it for _sentiment analysis_ using an appropropriate data set.  We will be using a small GPU such as the T4 using the [PyTorch](https://pytorch.org/) deep-learning framework.\n",
        "\n",
        "We will see two different ways to perform this task:\n",
        "\n",
        "1.  Using a Hugging Face class that is specifically designed for a a sequence classification task\n",
        "2.  Using a custom-written class \n",
        "\n",
        "\n",
        "## Prerequisites\n",
        "To run this lab you need to have the following:\n",
        "* An AML GPU-based compute instance\n",
        "* A conda environment named `bert_ft`, created using the enclosed `1_setup_conda.sh` file\n",
        "\n",
        "## Tools Used\n",
        "The Python tools used in this lab are the following open-source Hugging Face tools:\n",
        "\n",
        "* [Transformers](https://huggingface.co/docs/transformers/v4.17.0/en/index) - Implementation of a number of deep-learning models using the Transformer architecture\n",
        "\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "/mnt/batch/tasks/shared/LS_root/mounts/clusters/t4-instance/code/Users/yuvalmazor/FT BERT\r\n"
        }
      ],
      "execution_count": 53,
      "metadata": {}
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports & Definitions\n",
        "In this section we will import the classes we need and setup some definitions to be used later on."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertModel, BertTokenizer, BertForSequenceClassification\n",
        "import datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import random\n",
        "import pandas as pd\n",
        "\n",
        "pd.options.display.max_colwidth = 100\n",
        "\n",
        "# The pre-trained BERT model name\n",
        "model_name      = \"google-bert/bert-base-uncased\"\n",
        "\n",
        "# The sentiment analysis dataset used for fine-tuning\n",
        "dataset_name    = \"stanfordnlp/sentiment140\"\n",
        "\n",
        "# Num of instances from dataset to use for fine-tuning\n",
        "num_data_samples = 5000\n",
        "\n",
        "# GPU Device \n",
        "device = 'cuda:0'\n",
        "\n",
        "# Random seed\n",
        "seed_val = 42\n",
        "\n",
        "# Evaluation texts for the testing the fine-tuned model for sentiment\n",
        "evaluation_texts = [\n",
        "    'The critics praised this movie but I hated it!', \n",
        "    'I loved the new restaurant despite its bad service.',\n",
        "]\n",
        "\n",
        "\n",
        "# The following are taken from the BERT paper (https://arxiv.org/abs/1810.04805)\n",
        "\n",
        "# Batch size for fine-tuning \n",
        "batch_size = 32\n",
        "\n",
        "# Number of epochs for fine-tuning\n",
        "epochs=2\n",
        "\n",
        "# Learning rate \n",
        "learning_rate = 2e-5\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 54,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736067232497
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and Prepare the Data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "\n",
        "# Download the data and sample\n",
        "dataset = datasets.load_dataset(dataset_name)\n",
        "df = dataset['train'].to_pandas().sample(num_data_samples)\n",
        "\n",
        "labels = df['sentiment'].apply(lambda x: 0 if x == 0 else 1)\n",
        "\n",
        "df[['text', 'sentiment']].head(10)"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 55,
          "data": {
            "text/plain": "                                                                                                        text  \\\n541200                                                                @chrishasboobs AHHH I HOPE YOUR OK!!!    \n750                                                @misstoriblack cool , i have no tweet apps  for my razr 2   \n766711   @TiannaChaos i know  just family drama. its lame.hey next time u hang out with kim n u guys like...   \n285055           School email won't open  and I have geography stuff on there to revise! *Stupid School* :'(   \n705995                                                                                upper airways problem    \n379611                                                            Going to miss Pastor's sermon on Faith...    \n1189018                                                              on lunch....dj should come eat with me    \n667030                                                      @piginthepoke oh why are you feeling like that?    \n93541                                                        gahh noo!peyton needs to live!this is horrible    \n1097326  @mrstessyman thank you glad you like it! There is a product review bit on the site  Enjoy knitti...   \n\n         sentiment  \n541200           0  \n750              0  \n766711           0  \n285055           0  \n705995           0  \n379611           0  \n1189018          4  \n667030           0  \n93541            0  \n1097326          4  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>541200</th>\n      <td>@chrishasboobs AHHH I HOPE YOUR OK!!!</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>750</th>\n      <td>@misstoriblack cool , i have no tweet apps  for my razr 2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>766711</th>\n      <td>@TiannaChaos i know  just family drama. its lame.hey next time u hang out with kim n u guys like...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>285055</th>\n      <td>School email won't open  and I have geography stuff on there to revise! *Stupid School* :'(</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>705995</th>\n      <td>upper airways problem</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>379611</th>\n      <td>Going to miss Pastor's sermon on Faith...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1189018</th>\n      <td>on lunch....dj should come eat with me</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>667030</th>\n      <td>@piginthepoke oh why are you feeling like that?</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>93541</th>\n      <td>gahh noo!peyton needs to live!this is horrible</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1097326</th>\n      <td>@mrstessyman thank you glad you like it! There is a product review bit on the site  Enjoy knitti...</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 55,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736067236957
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoding our data in preparation for fine-tuning\n",
        "\n",
        "encoded_texts = tokenizer.batch_encode_plus(\n",
        "    df['text'].values,                  # Data is in the `text` column\n",
        "    add_special_tokens=True,            # Make sure to wrap the text in [CLS] and [SEP] tokens\n",
        "    max_length=64,                      # Maximum length for a single sentence\n",
        "    padding='longest',                  # Padding strategy:  pad to max_length\n",
        "    return_attention_mask=True,         # Return attention mask:  Valid vs. padding tokens\n",
        "    return_tensors='pt'                 # Return as PyTorch tensors\n",
        ")\n",
        "\n",
        "# Prepare PyTorch Dataset \n",
        "\n",
        "dataset = torch.utils.data.TensorDataset(\n",
        "    encoded_texts['input_ids'],         # Encoded data\n",
        "    encoded_texts['attention_mask'],    # Attention masks \n",
        "    torch.Tensor(labels.values).long()  # Sentiment values as long values\n",
        ")"
      ],
      "outputs": [],
      "execution_count": 56,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736067239043
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quick Introdcution to BERT \n",
        "\n",
        "[BERT (Bidirectional Encoder Representations from Transformers, Devlin et al, 2018)](https://arxiv.org/abs/1810.04805) is a language model introduced in 2018 by Google researchers for generating text representations using the [Transformer architecture](https://arxiv.org/abs/1706.03762).  By encoding text in multiple directions (hence the 'bidirectional' name) it is able to generate very powerful representations - known as 'embeddings' - which can then be used for further downstream tasks such as classification or question answering.\n",
        "\n",
        "Text is encoded into BERT in the following manner:\n",
        "\n",
        "[[ CLS ]] token_1 token_2 ... token_n [[ SEP ]] token_n+1 ... token_m [[ SEP ]]\n",
        "\n",
        "where CLS is the _class token_ that enables BERT to be used for classification tasks and SEP is a _separator_ token allowing BERT to optionally contain 2 sentences in each input.  BERT is originally designed to have a maximum of 512 tokens per input.\n",
        "\n",
        "For sentiment analysis we will fine-tune BERT with a custom dataset so that the `CLS` token to have a score between 0 (negative sentiment) and 1 (positive sentiment).\n"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## First Approach:  Fine-Tuning BERT with Hugging Face Classes\n",
        "\n",
        "In this section we will fine tune the pretrained BERT model using HuggingFace's `BertForSequenceClassficiation` class.  This class extends the plain BERT model class to support an additional classification head on top of BERT, composed of a feed-forward and a dropout layer.  We will then try the model with the evaulation texts from above."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "\n",
        "# Prepare the pretrained model\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    model_name,                       # The pretrained model\n",
        "    num_labels=len(labels.unique()),  # Number of labels for the classification task\n",
        "    output_attentions=False,          \n",
        "    output_hidden_states = False, \n",
        "    ).to(device)\n",
        "\n",
        "# Set up optimizer and data loader\n",
        "adam = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(epochs):\n",
        "    print (f\"Epoch {epoch}\\n---------\")\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader):\n",
        "        input_ids, attention_masks, labels = (batch[i].to(device) for i in range(len(batch)))\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "        # Note the the HF classes are able to calculate loss themselves\n",
        "        loss, logits = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels, return_dict=False)\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        adam.step()\n",
        "\n",
        "        if batch_idx % 40 == 0:\n",
        "            print(f\"Loss for batch {batch_idx}: \", end=\"\")\n",
        "            print(loss.item())\n",
        "    print(\"\\n\")\n",
        "    print(f\"Average loss: {np.mean(losses)}\\n\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at google-bert/bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 0\n---------\nLoss for batch 0: 0.6926805377006531\nLoss for batch 40: 0.5924568176269531\nLoss for batch 80: 0.4777229428291321\nLoss for batch 120: 0.46647247672080994\n\n\nAverage loss: 0.5119854244077282\n\nEpoch 1\n---------\nLoss for batch 0: 0.36755692958831787\nLoss for batch 40: 0.3337683379650116\nLoss for batch 80: 0.4166205823421478\nLoss for batch 120: 0.29434990882873535\n\n\nAverage loss: 0.31982326730611216\n\n"
        }
      ],
      "execution_count": 59,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736067656396
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "eval_df = pd.DataFrame(data=evaluation_texts, columns=['text'])\n",
        "\n",
        "with torch.no_grad():\n",
        "    encoded_texts = tokenizer.batch_encode_plus(\n",
        "        eval_df['text'],\n",
        "        add_special_tokens=True, max_length=64, \n",
        "        padding='longest', \n",
        "        return_attention_mask=True, \n",
        "        return_tensors='pt')\n",
        "    \n",
        "    # The Hugging Face classes return a number of different outputs - we need the sequence in the first\n",
        "    # output\n",
        "    eval_results = model(encoded_texts['input_ids'].to(device), return_dict=False)[0]\n",
        "    eval_results = F.softmax(eval_results, dim=1).cpu().numpy()\n",
        "\n",
        "    eval_df['Negative'] = eval_results[:, 0]\n",
        "    eval_df['Positive'] = eval_results[:, 1]\n",
        "\n",
        "eval_df"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 60,
          "data": {
            "text/plain": "                                                  text  Negative  Positive\n0       The critics praised this movie but I hated it!  0.977916  0.022084\n1  I loved the new restaurant despite its bad service.  0.087739  0.912261",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Negative</th>\n      <th>Positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The critics praised this movie but I hated it!</td>\n      <td>0.977916</td>\n      <td>0.022084</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I loved the new restaurant despite its bad service.</td>\n      <td>0.087739</td>\n      <td>0.912261</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 60,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736067690066
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Second Approach:  Fine-Tuning BERT with Custom Classes\n",
        "\n",
        "In this section we will fine tune the pretrained BERT using a custom-written class.  This gives us more flexibility and we will also create our own feed-foward and dropout layers on top of the existing model."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Our custom model - using 2 Linear layers and dropout\n",
        "class CustomModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.bert = BertModel.from_pretrained(model_name)\n",
        "        self.linear1 = nn.Linear(768, 10)\n",
        "        self.linear2 = nn.Linear(10, 2)\n",
        "        self.dropout = nn.Dropout(0.3)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, return_dict=None, labels=None):\n",
        "        ret = self.bert(input_ids=input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, return_dict=return_dict)\n",
        "        pooler_output = ret[1]\n",
        "        ret = F.gelu(self.linear1(pooler_output))\n",
        "        ret = self.dropout(ret)\n",
        "        return F.softmax(self.linear2(ret), dim=1)\n",
        "\n",
        "model = CustomModel().to(device)\n",
        "\n",
        "adam = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "loss_fn = nn.CrossEntropyLoss()\n",
        "loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print (f\"Epoch {epoch}\\n---------\")\n",
        "    model.train()\n",
        "    losses = []\n",
        "\n",
        "    for batch_idx, batch in enumerate(loader):\n",
        "        input_ids, attention_masks, labels = (batch[i].to(device) for i in range(len(batch)))\n",
        "        \n",
        "        model.zero_grad()\n",
        "\n",
        "        # With a custom class it is up to us to calculate loss for each batch\n",
        "        res = model(input_ids, token_type_ids=None, attention_mask=attention_masks, labels=labels, return_dict=False)\n",
        "        loss=loss_fn(res, labels)\n",
        "        \n",
        "        losses.append(loss.item())\n",
        "        loss.backward()\n",
        "        adam.step()\n",
        "\n",
        "        if batch_idx % 40 == 0:\n",
        "            print(f\"Loss for batch {batch_idx}: \", end=\"\")\n",
        "            print(loss.item())\n",
        "    print(\"\\n\")\n",
        "    print(f\"Average loss: {np.mean(losses)}\\n\")\n",
        "\n",
        "   "
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Epoch 0\n---------\nLoss for batch 0: 0.7031998634338379\nLoss for batch 40: 0.5776113867759705\nLoss for batch 80: 0.535115659236908\nLoss for batch 120: 0.5173466205596924\n\n\nAverage loss: 0.5989916662501681\n\nEpoch 1\n---------\nLoss for batch 0: 0.4463011622428894\nLoss for batch 40: 0.5075926184654236\nLoss for batch 80: 0.5965098142623901\nLoss for batch 120: 0.5559391975402832\n\n\nAverage loss: 0.50429661391647\n\n"
        }
      ],
      "execution_count": 62,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736068311453
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "eval_df = pd.DataFrame(data=evaluation_texts, columns=['text'])\n",
        "\n",
        "with torch.no_grad():\n",
        "    encoded_texts = tokenizer.batch_encode_plus(\n",
        "        eval_df['text'],\n",
        "        add_special_tokens=True, max_length=64, \n",
        "        padding='longest', \n",
        "        return_attention_mask=True, \n",
        "        return_tensors='pt')\n",
        "    \n",
        "    # Our custom class returns the precise outputs we're interested in\n",
        "    eval_results = model(encoded_texts['input_ids'].to(device), return_dict=False)\n",
        "    eval_results = F.softmax(eval_results, dim=1).cpu().numpy()\n",
        "\n",
        "    eval_df['Negative'] = eval_results[:, 0]\n",
        "    eval_df['Positive'] = eval_results[:, 1]\n",
        "\n",
        "eval_df\n",
        "\n"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 75,
          "data": {
            "text/plain": "                                                  text  Negative  Positive\n0       The critics praised this movie but I hated it!  0.725574  0.274426\n1  I loved the new restaurant despite its bad service.  0.448317  0.551683",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>Negative</th>\n      <th>Positive</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>The critics praised this movie but I hated it!</td>\n      <td>0.725574</td>\n      <td>0.274426</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I loved the new restaurant despite its bad service.</td>\n      <td>0.448317</td>\n      <td>0.551683</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 75,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1736068618870
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "llama3_ft",
      "language": "python",
      "display_name": "Python (bert_ft)"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.16",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "llama3_ft"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}